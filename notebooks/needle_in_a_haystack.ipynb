{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b570074e-cda9-4c86-bffc-b2cd35468884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kvpress import AdapPress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d33948a0-4510-4a88-a518-d0a4f084d179",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_ratio = 0.3\n",
    "press = AdapPress(compression_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c9d382f-04e6-4b96-b47e-2135246c4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kvpress import SnapKVPress\n",
    "from kvpress import KnormPress\n",
    "compression_ratio = 0.3\n",
    "press = KnormPress(compression_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2fbbbc7-21c6-4593-abc3-d995c1c69297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdapPress\n"
     ]
    }
   ],
   "source": [
    "print(f\"{press.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3119e5e-c2c6-49db-8760-8554e1fbfe78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "Model loaded on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Qwen/Qwen2.5-0.5B-Instruct:   0%|                                                                                                                             | 0/5 [00:00<?, ?it/s]/home/nasno/Project/kvpress/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/nasno/Project/kvpress/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/nasno/Project/kvpress/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Testing Qwen/Qwen2.5-0.5B-Instruct: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [08:14<00:00, 98.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created heatmap for Qwen/Qwen2.5-0.5B-Instruct\n",
      "All tests and visualizations completed. Results saved in 'needle_test_results_20250501_105108'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def generate_haystack(length, needle_text, needle_position_ratio=None):\n",
    "    \"\"\"\n",
    "    Generate a haystack text with a needle inserted at a specific position.\n",
    "    \n",
    "    Args:\n",
    "        length: Length of the haystack in characters\n",
    "        needle_text: The needle text to be inserted\n",
    "        needle_position_ratio: Position of the needle as a ratio of the total length (0.0-1.0)\n",
    "                              If None, a random position is chosen\n",
    "    \n",
    "    Returns:\n",
    "        haystack_text: The combined text\n",
    "        needle_position: The character position where the needle starts\n",
    "    \"\"\"\n",
    "    # Generate random text for the haystack\n",
    "    haystack_chars = ''.join(random.choices(string.ascii_letters + string.digits + ' ' * 5 + ',.!?', k=length))\n",
    "    \n",
    "    # Determine needle position\n",
    "    if needle_position_ratio is None:\n",
    "        needle_position_ratio = random.uniform(0.2, 0.8)  # Avoid extreme ends\n",
    "    \n",
    "    needle_position = int(needle_position_ratio * (length - len(needle_text)))\n",
    "    \n",
    "    # Insert the needle\n",
    "    haystack_text = haystack_chars[:needle_position] + needle_text + haystack_chars[needle_position:length-len(needle_text)]\n",
    "    \n",
    "    return haystack_text, needle_position\n",
    "\n",
    "def run_needle_test(model, tokenizer, haystack_text, question, needle_value, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Run a needle-in-a-haystack test with a transformer model.\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        tokenizer: The corresponding tokenizer\n",
    "        haystack_text: The haystack text containing the needle\n",
    "        question: The question to ask to retrieve the needle\n",
    "        needle_value: The expected answer\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results including response, success, similarity, etc.\n",
    "    \"\"\"\n",
    "    prompt = f\"{haystack_text}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    \n",
    "    # Tokenize and get token count\n",
    "    tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    token_count = tokens.input_ids.shape[1]\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokens = {key: val.to(device) for key, val in tokens.items()}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad(), press(model):\n",
    "        # with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                tokens[\"input_ids\"],\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0][tokens[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        # Evaluate success\n",
    "        from difflib import SequenceMatcher\n",
    "        \n",
    "        # Simple exact or partial match check\n",
    "        if needle_value.lower() in response.lower():\n",
    "            success = True\n",
    "            similarity = 1.0\n",
    "        else:\n",
    "            # Partial match using character-level similarity\n",
    "            similarity = SequenceMatcher(None, response.lower(), needle_value.lower()).ratio()\n",
    "            success = similarity > 0.8  # Adjust threshold as needed\n",
    "        \n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            \"response\": response,\n",
    "            \"success\": success,\n",
    "            \"similarity\": similarity,\n",
    "            \"token_count\": token_count,\n",
    "            \"inference_time\": inference_time\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {str(e)}\")\n",
    "        return {\n",
    "            \"response\": f\"ERROR: {str(e)}\",\n",
    "            \"success\": False,\n",
    "            \"similarity\": 0,\n",
    "            \"token_count\": token_count,\n",
    "            \"inference_time\": time.time() - start_time\n",
    "        }\n",
    "\n",
    "def test_model_across_depths_and_lengths(model_name, context_lengths, depth_positions, test_repeats=2):\n",
    "    \"\"\"\n",
    "    Test a model across different context lengths and document depths.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model to test\n",
    "        context_lengths: List of context lengths to test (in characters)\n",
    "        depth_positions: List of depth positions to test (as ratios 0.0-1.0)\n",
    "        test_repeats: Number of times to repeat each test for reliability\n",
    "    \n",
    "    Returns:\n",
    "        results_df: DataFrame containing all test results\n",
    "    \"\"\"\n",
    "    print(f\"Testing model: {model_name}\")\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "        print(f\"Model loaded on {device}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model {model_name}: {str(e)}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Define a fixed needle value for consistent testing\n",
    "    secret_value = \"XYZ42\"\n",
    "    needle_text = f\"The secret code is {secret_value}.\"\n",
    "    question = \"What is the secret code?\"\n",
    "    \n",
    "    # Loop through all test configurations\n",
    "    for length in tqdm(context_lengths, desc=f\"Testing {model_name}\"):\n",
    "        for depth in depth_positions:\n",
    "            for repeat in range(test_repeats):\n",
    "                # Generate haystack with needle\n",
    "                haystack_text, needle_position = generate_haystack(\n",
    "                    length, \n",
    "                    needle_text, \n",
    "                    depth\n",
    "                )\n",
    "                \n",
    "                # Approximate token count (rough estimate)\n",
    "                char_to_token_ratio = 4  # Approximate ratio of characters to tokens\n",
    "                token_length_estimate = length // char_to_token_ratio\n",
    "                \n",
    "                # Skip if estimated tokens exceed model's known limits\n",
    "                if token_length_estimate > 8192 and \"gpt2\" in model_name:  # Example limit for GPT-2\n",
    "                    result = {\n",
    "                        \"model\": model_name,\n",
    "                        \"context_length_chars\": length,\n",
    "                        \"context_length_tokens_est\": token_length_estimate,\n",
    "                        \"depth_position\": depth,\n",
    "                        \"depth_percent\": int(depth * 100),  # Add percentage for better visualization\n",
    "                        \"repeat\": repeat,\n",
    "                        \"success\": False,\n",
    "                        \"similarity\": 0,\n",
    "                        \"response\": \"SKIPPED - Context too long for model\",\n",
    "                        \"inference_time\": 0\n",
    "                    }\n",
    "                else:\n",
    "                    # Run the test\n",
    "                    test_result = run_needle_test(\n",
    "                        model=model, \n",
    "                        tokenizer=tokenizer, \n",
    "                        haystack_text=haystack_text, \n",
    "                        question=question,\n",
    "                        needle_value=secret_value\n",
    "                    )\n",
    "                    \n",
    "                    # Compile result\n",
    "                    result = {\n",
    "                        \"model\": model_name,\n",
    "                        \"context_length_chars\": length,\n",
    "                        \"context_length_tokens_est\": token_length_estimate,\n",
    "                        \"context_length_tokens_actual\": test_result[\"token_count\"],\n",
    "                        \"depth_position\": depth,\n",
    "                        \"depth_percent\": int(depth * 100),  # Add percentage for better visualization\n",
    "                        \"repeat\": repeat,\n",
    "                        \"success\": test_result[\"success\"],\n",
    "                        \"similarity\": test_result[\"similarity\"],\n",
    "                        \"score\": int(test_result[\"similarity\"] * 100),  # Add score as percentage\n",
    "                        \"response\": test_result[\"response\"],\n",
    "                        \"inference_time\": test_result[\"inference_time\"]\n",
    "                    }\n",
    "                \n",
    "                results.append(result)\n",
    "                \n",
    "                # Save intermediate results to avoid losing progress\n",
    "                if len(results) % 5 == 0:\n",
    "                    pd.DataFrame(results).to_csv(f\"{model_name.replace('/', '_')}_intermediate_results.csv\", index=False)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save full results\n",
    "    results_df.to_csv(f\"{model_name.replace('/', '_')}_results.csv\", index=False)\n",
    "    \n",
    "    # Free up GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def create_heatmap_visualization(results_df, output_dir=\"results\"):\n",
    "    \"\"\"\n",
    "    Create a heatmap visualization for each model in the results using the specified color scheme.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame containing test results\n",
    "        output_dir: Directory to save visualizations\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get unique models\n",
    "    models = results_df[\"model\"].unique()\n",
    "    \n",
    "    # Define the custom color map as specified\n",
    "    cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", [\"#F0496E\", \"#EBB839\", \"#0CD79F\"])\n",
    "    \n",
    "    for model_name in models:\n",
    "        model_df = results_df[results_df[\"model\"] == model_name]\n",
    "        \n",
    "        # Create a pivot table for the heatmap\n",
    "        # pivot_table = model_df.pivot_table(\n",
    "        #     values=\"score\",  # Use percentage score\n",
    "        #     index=\"depth_percent\",  # Y-axis: depth as percentage\n",
    "        #     columns=\"context_length_tokens_est\",  # X-axis: tokens\n",
    "        #     aggfunc=\"mean\"  # Average score for each combination\n",
    "        # )\n",
    "        pivot_table = pd.pivot_table(model_df, values='score', index=['depth_percent', 'context_length_chars'], aggfunc='mean').reset_index() # This will aggregate\n",
    "        pivot_table = pivot_table.pivot(index=\"depth_percent\", columns=\"context_length_chars\", values=\"score\") # This will turn into a proper pivot\n",
    "        \n",
    "        # Create the heatmap with better aesthetics\n",
    "        plt.figure(figsize=(17.5, 8))\n",
    "        ax = sns.heatmap(\n",
    "            pivot_table,\n",
    "            # annot=True,  # Show values in cells\n",
    "            fmt=\"g\",  # General format for integers\n",
    "            cmap=cmap,\n",
    "            vmin=0,\n",
    "            vmax=100,\n",
    "            cbar_kws={'label': 'Score'}\n",
    "        )\n",
    "        \n",
    "        # Improve aesthetics\n",
    "        clean_model_name = model_name.replace(\"/\", \"-\")\n",
    "        plt.title(f'Pressure Testing {clean_model_name}\\nFact Retrieval Across Context Lengths (\"Needle In A HayStack\")', fontsize=16)\n",
    "        plt.xlabel('Token Length')\n",
    "        plt.ylabel('Depth Percentage')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=0)\n",
    "        \n",
    "        # Add annotations to cells that are below 80% (indicating potential issues)\n",
    "        for i in range(pivot_table.shape[0]):\n",
    "            for j in range(pivot_table.shape[1]):\n",
    "                if not np.isnan(pivot_table.iloc[i, j]) and pivot_table.iloc[i, j] < 80:\n",
    "                    ax.text(j + 0.5, i + 0.5, f\"{int(pivot_table.iloc[i, j])}%\", \n",
    "                            ha='center', va='center', color='white', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the figure\n",
    "        plt.savefig(f\"{output_dir}/{clean_model_name}_heatmap_{press.__class__.__name__}_compression{compression_ratio}.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Created heatmap for {model_name}\")\n",
    "\n",
    "def compare_models_visualization(results_df, output_dir=\"results\"):\n",
    "    \"\"\"\n",
    "    Create visualizations comparing models across different dimensions.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame containing test results\n",
    "        output_dir: Directory to save visualizations\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get unique models\n",
    "    models = results_df[\"model\"].unique()\n",
    "    \n",
    "    # 1. Success rate by context length for each model\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for model_name in models:\n",
    "        model_df = results_df[results_df[\"model\"] == model_name]\n",
    "        \n",
    "        # Group by context length and calculate success rate\n",
    "        success_by_length = model_df.groupby(\"context_length_tokens_est\")[\"score\"].mean()\n",
    "        \n",
    "        plt.plot(success_by_length.index, success_by_length.values, \n",
    "                marker='o', linewidth=2, label=model_name)\n",
    "    \n",
    "    plt.title(\"Model Performance by Context Length\", fontsize=16)\n",
    "    plt.xlabel(\"Context Length (tokens)\")\n",
    "    plt.ylabel(\"Score (%)\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f\"{output_dir}/model_comparison_by_length.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Success rate by depth position for each model\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for model_name in models:\n",
    "        model_df = results_df[results_df[\"model\"] == model_name]\n",
    "        \n",
    "        # Group by depth position and calculate success rate\n",
    "        success_by_depth = model_df.groupby(\"depth_percent\")[\"score\"].mean()\n",
    "        \n",
    "        plt.plot(success_by_depth.index, success_by_depth.values, \n",
    "                marker='o', linewidth=2, label=model_name)\n",
    "    \n",
    "    plt.title(\"Model Performance by Document Depth\", fontsize=16)\n",
    "    plt.xlabel(\"Document Depth (%)\")\n",
    "    plt.ylabel(\"Score (%)\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f\"{output_dir}/model_comparison_by_depth.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Create a combined heatmap comparison for all models\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # Get unique depth percentages and token lengths\n",
    "    depths = sorted(results_df[\"depth_percent\"].unique())\n",
    "    lengths = sorted(results_df[\"context_length_tokens_est\"].unique())\n",
    "    \n",
    "    # Define subplot grid\n",
    "    n_models = len(models)\n",
    "    cols = min(3, n_models)\n",
    "    rows = (n_models + cols - 1) // cols\n",
    "    \n",
    "    # Create the custom color map\n",
    "    cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", [\"#F0496E\", \"#EBB839\", \"#0CD79F\"])\n",
    "    \n",
    "    for i, model_name in enumerate(models):\n",
    "        model_df = results_df[results_df[\"model\"] == model_name]\n",
    "        \n",
    "        # Create pivot table\n",
    "        pivot_table = model_df.pivot_table(\n",
    "            values=\"score\",\n",
    "            index=\"depth_percent\",\n",
    "            columns=\"context_length_tokens_est\",\n",
    "            aggfunc=\"mean\"\n",
    "        )\n",
    "        \n",
    "        # Add subplot\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        \n",
    "        # Create heatmap\n",
    "        sns.heatmap(\n",
    "            pivot_table,\n",
    "            annot=True,\n",
    "            fmt=\"g\",\n",
    "            cmap=cmap,\n",
    "            cbar_kws={'label': 'Score (%)'}\n",
    "        )\n",
    "        \n",
    "        plt.title(model_name)\n",
    "        plt.xlabel('Token Length')\n",
    "        plt.ylabel('Depth (%)')\n",
    "        \n",
    "    plt.show()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/all_models_comparison.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Created model comparison visualizations\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the needle-in-haystack tests and create visualizations.\"\"\"\n",
    "    \n",
    "    # Define models to test\n",
    "    models = [\n",
    "        \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    ]\n",
    "    \n",
    "    # Define test parameters with more depth positions as requested\n",
    "    context_lengths = [1000, 2000, 5000, 10000, 20000]  # Characters\n",
    "    # More depth positions (10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%)\n",
    "    depth_positions = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    test_repeats = 2  # Number of times to repeat each test\n",
    "    \n",
    "    # Create results directory\n",
    "    output_dir = f\"needle_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Run tests for each model\n",
    "    all_results = []\n",
    "    \n",
    "    for model_name in models:\n",
    "        model_results = test_model_across_depths_and_lengths(\n",
    "            model_name, \n",
    "            context_lengths, \n",
    "            depth_positions, \n",
    "            test_repeats\n",
    "        )\n",
    "        all_results.append(model_results)\n",
    "    \n",
    "    # Combine all results\n",
    "    combined_results = pd.concat(all_results, ignore_index=True)\n",
    "    combined_results.to_csv(f\"{output_dir}/all_models_results.csv\", index=False)\n",
    "    \n",
    "    # Create visualizations\n",
    "    create_heatmap_visualization(combined_results, output_dir)\n",
    "    # compare_models_visualization(combined_results, output_dir)\n",
    "    \n",
    "    print(f\"All tests and visualizations completed. Results saved in '{output_dir}'\")\n",
    "    \n",
    "    # Return the results for further analysis if needed\n",
    "    return combined_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311338f-856a-46dd-916a-0d8e5aedd21e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
